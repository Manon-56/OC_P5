{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1196a74",
   "metadata": {},
   "source": [
    "# Projet 5 - Segmentez des clients d'un site e-commerce - exploration de différents modèles de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c37d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST AVEC DONNÉES MINIMALES ===\n",
      "Taille dataset test: (30000, 2)\n",
      "Mémoire: 283.8 MB\n",
      "Mémoire système libre: 12505.5 MB\n",
      "Test DBSCAN...\n",
      "# test\n",
      "Avec les paramètres choisis, DBSCAN a défini 1 clusters\n",
      "Il y a 0.0% d'outliers\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import gc\n",
    "import os \n",
    "\n",
    "def monitor_resources():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    print(f\"Mémoire: {memory_mb:.1f} MB\")\n",
    "    print(f\"Mémoire système libre: {psutil.virtual_memory().available / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Test avec données minimales\n",
    "print(\"=== TEST AVEC DONNÉES MINIMALES ===\")\n",
    "try:\n",
    "    from custom_library import outils\n",
    "    \n",
    "    # Créez un très petit dataset de test\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Dataset minimal pour le test\n",
    "    test_data = pd.DataFrame({\n",
    "        'feature1': np.random.rand(30000),  # Seulement 50 points !\n",
    "        'feature2': np.random.rand(30000)\n",
    "    })\n",
    "    \n",
    "    print(f\"Taille dataset test: {test_data.shape}\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Test DBSCAN avec paramètres conservateurs\n",
    "    print(\"Test DBSCAN...\")\n",
    "    result_dbscan = outils.prepare_compute_evaluate_dbscan(\n",
    "        test_data, \n",
    "        eps=0.5,  # Paramètres simples\n",
    "        min_samples=5,\n",
    "        data_description = \"test\"\n",
    "    )\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Nettoyage mémoire entre les tests\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Test KMeans...\")\n",
    "    result_kmeans = outils.prepare_compute_evaluate_kmeans(\n",
    "        test_data,\n",
    "        max_num_clusters=3,  # Petit nombre de clusters\n",
    "        data_description = \"test\",\n",
    "        random_state = 42\n",
    "    )\n",
    "    monitor_resources()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur: {e}\")\n",
    "    monitor_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from math import pi\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from hdbscan.validity import validity_index\n",
    "\n",
    "\n",
    "from custom_library.outils import prepare_compute_evaluate_kmeans, prepare_compute_evaluate_dbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf80f71",
   "metadata": {},
   "source": [
    "## Importation des données nettoyées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c144daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clean_dataset.pkl\",\"rb\") as f:\n",
    "\tdata = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f199179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95082, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c49ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nb_commandes', 'nb_produits', 'depense_totale', 'recence',\n",
       "       'score_moyen', 'cat_Alimentation & Boissons', 'cat_Animaux',\n",
       "       'cat_Auto & Transport', 'cat_Autres', 'cat_Bricolage & Jardin',\n",
       "       'cat_Bébé & Enfants', 'cat_Cuisine & Accessoires décoratifs',\n",
       "       'cat_Loisirs & Culture', 'cat_Mobilier & Aménagement intérieur',\n",
       "       'cat_Mode & Accessoires', 'cat_Papeterie & Bureau',\n",
       "       'cat_Santé & Beauté', 'cat_Électroménager',\n",
       "       'cat_Électronique & Informatique', 'cat_Centre-Ouest', 'cat_Nord',\n",
       "       'cat_Nord-Est', 'cat_Sud', 'cat_Sud-Est'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403c2f5",
   "metadata": {},
   "source": [
    "## Diviser en 4 différents jeux de données : \n",
    "- données RFM i.e. qui contiennent seulement les variables de récence (\"recence\"), de fréquence (\"nb_commandes\") et de montant (\"depense_totale\")\n",
    "- données numériques i.e. qui contiennent seulement les variables numériques, et pas les variables catégorielles encodées\n",
    "- données totales\n",
    "- données numériques sans récence car elle n'a pas beaucoup de sens pour notre jeu de données sachant que 96% des clients n'ont passé qu'une seule commande. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98f97c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant d'enlever les valeurs manquantes, le dataset contenait 95082 clients\n",
      "Après avoir enlevé les valeurs manquantes du dataset data_RFM, il reste 95082 clients\n",
      "Après avoir enlevé les valeurs manquantes du dataset data_numeric, il reste 94385 clients\n",
      "Après avoir enlevé les valeurs manquantes du dataset data_num_WO_recency, il reste 94385 clients\n",
      "Après avoir enlevé les valeurs manquantes du dataset data_tot, il reste 94385 clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3956/1580952926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_RFM.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avant d'enlever les valeurs manquantes, le dataset contenait {len(data)} clients\")\n",
    "\n",
    "data_RFM = data[['recence','nb_commandes','depense_totale']]\n",
    "data_RFM.dropna(inplace=True)\n",
    "print(f\"Après avoir enlevé les valeurs manquantes du dataset data_RFM, il reste {len(data_RFM)} clients\")\n",
    "\n",
    "data_numeric = data.iloc[:,0:5]\n",
    "data_numeric.dropna(inplace=True)\n",
    "print(f\"Après avoir enlevé les valeurs manquantes du dataset data_numeric, il reste {len(data_numeric)} clients\")\n",
    "\n",
    "data_num_WO_recency = data_numeric.drop('recence',axis = 1)\n",
    "data_num_WO_recency.dropna(inplace=True)\n",
    "print(f\"Après avoir enlevé les valeurs manquantes du dataset data_num_WO_recency, il reste {len(data_num_WO_recency)} clients\")\n",
    "\n",
    "data_tot = data\n",
    "data_tot.dropna(inplace=True)\n",
    "print(f\"Après avoir enlevé les valeurs manquantes du dataset data_tot, il reste {len(data_tot)} clients\")\n",
    "\n",
    "datasets_list = [data_RFM,data_numeric,data_num_WO_recency,data_tot]\n",
    "datasets_descriptions = [\"Variables RFM seules\", \"Variables numériques seules\", \"Variables numériques sans la variables récence\", \"Variables numériques et catégorielles encodées\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c796e9a",
   "metadata": {},
   "source": [
    "## Méthode du Kmeans\n",
    "\n",
    "K-Means est un algorithme de **clustering** non supervisé qui cherche à partitionner les données en `k` groupes (ou \"clusters\") en minimisant la **variance intra-cluster**. Le processus se déroule en plusieurs étapes :\n",
    "\n",
    "1. Initialisation de `k` centroïdes (aléatoirement ou par méthode heuristique).\n",
    "2. Attribution de chaque point au centroïde le plus proche (selon la distance euclidienne).\n",
    "3. Recalcul des centroïdes comme moyenne des points de chaque cluster.\n",
    "4. Répétition des étapes 2 et 3 jusqu’à convergence (peu ou pas de changement dans les centroïdes).\n",
    "\n",
    "Plusieurs **métriques** permettent d’évaluer la qualité du regroupement et d’aider à choisir un `k` pertinent :\n",
    "\n",
    "1. L'inertie\n",
    "- Mesure la variance intracluster\n",
    "- Inertie minimale est signe de clusters denses\n",
    "\n",
    "2. Méthode du coude (Elbow method)\n",
    "- Repose sur la mesure de la **somme des distances au carré** entre les points et leur centroïde (inertie intra-cluster).\n",
    "- On trace l'inertie en fonction de `k`. Le **\"coude\"** sur la courbe indique un bon compromis entre complexité du modèle et performance.\n",
    "\n",
    "3. Coefficient de silhouette\n",
    "- Mesure la **similarité d’un point avec son propre cluster** comparée à celui le plus proche.\n",
    "- Le score varie entre -1 (mauvais regroupement) et 1 (regroupement optimal).\n",
    "- Une moyenne élevée indique que les clusters sont bien séparés et denses.\n",
    "\n",
    "4. Indice de Davies-Bouldin\n",
    "- Évalue la **compacité et la séparation** des clusters.\n",
    "- Repose sur le ratio entre l’étalement intra-cluster et la distance inter-cluster --> plus l’indice est **faible**, meilleure est la qualité du regroupement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(datasets_list)):\n",
    "\tprepare_compute_evaluate_kmeans(datasets_list[idx], datasets_descriptions[idx], max_num_clusters = 8, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15127eb",
   "metadata": {},
   "source": [
    "La présence ou non de la variable récence dans le dataset composé des variables numériques seules n'a pas d'impact sur les résultats du kmeans. <br> L'ajout des variables encodées détériore fortement les résultats du kmeans. \n",
    "Le KMeans se base sur des distances euclidiennes, qui supposent une continuité et une échelle comparable entre les variables.\n",
    "L’ajout de variables catégorielles encodées en one-hot peut déséquilibrer les distances :\n",
    "- chaque catégorie introduit une nouvelle dimension binaire, ce qui augmente artificiellement l'influence de cette variable.\n",
    "- cela peut déformer l’espace de distances et dégrader fortement la qualité des clusters.\n",
    "Une solution pourrait être de changer la façon de calculer les distances en utilisant des équivalents au kmeans basées sur d'autres types de distances par exemple distance de manhattan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_choisi = 4\n",
    "data_chosen = data_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b577b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application finale du K-means avec le K choisi\n",
    "\t# Préparation des données\n",
    "pt = PowerTransformer(method=\"yeo-johnson\", standardize = True)\n",
    "X_transformed = pt.fit_transform(data_chosen)\n",
    "final_kmeans = KMeans(n_clusters=k_choisi, random_state=42)\n",
    "final_clusters = final_kmeans.fit_predict(X_transformed)\n",
    "\n",
    "# Affichage des scores finaux\n",
    "print(f\"\\nScores pour K = {k_choisi}:\")\n",
    "print(f\"Inertie: {final_kmeans.inertia_}\")\n",
    "print(f\"Score Silhouette: {silhouette_score(X_transformed, final_clusters)}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin_score(X_transformed, final_clusters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad424e1b",
   "metadata": {},
   "source": [
    "- data_numeric, random state = 42 : <br> \n",
    "Scores pour K = 4:\n",
    "    - Inertie: 79114.74181976692\n",
    "    - Score Silhouette: 0.4485005102704839\n",
    "    - Davies-Bouldin Index: 0.7360699869869303\n",
    "\n",
    "<!-- - data_numeric, random state = 12 : <br>\n",
    "Scores pour K = 4:\n",
    "\t- Inertie: 79114.54283500843\n",
    "\t- Score Silhouette: 0.4484665986665208\n",
    "\t- Davies-Bouldin Index: 0.7360158793413563\n",
    "\n",
    "--> le modèle est assez stable -->\n",
    "\n",
    "- data_RFM, random_state = 42: <br>\n",
    "Scores pour K = 4:\n",
    "\t- Inertie: 10403\n",
    "\t- Score Silhouette: 0.54\n",
    "\t- Davies-Bouldin Index: 0.54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a7c44",
   "metadata": {},
   "source": [
    "## Comparer les profils moyens des clusters sur chaque variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531eac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroïdes (dans l'espace standardisé si KMeans a été fait sur les données standardisées)\n",
    "centroids = pd.DataFrame(final_kmeans.cluster_centers_, columns=data_chosen.columns)\n",
    "\n",
    "# Barplot par cluster\n",
    "centroids.T.plot(kind=\"bar\", figsize=(12, 6))\n",
    "plt.title(\"Centroïdes des clusters (standardisés)\")\n",
    "plt.ylabel(\"Valeur moyenne par cluster\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65deefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(centroids.T, annot=False, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Heatmap des centroïdes\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca8f76",
   "metadata": {},
   "source": [
    "## Visualiser le profil complet de chaque cluster sur un cercle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar(centroids_df, data_orig_df, labels):\n",
    "    # Calcul des moyennes originales par cluster\n",
    "    original_means = data_orig_df.copy()\n",
    "    original_means[\"cluster\"] = labels\n",
    "    original_means = original_means.groupby(\"cluster\").median()\n",
    "\n",
    "    categories = centroids_df.columns.tolist()\n",
    "    N = len(categories)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for idx, row in centroids_df.iterrows():\n",
    "        values = row.tolist()\n",
    "        values += values[:1]\n",
    "        plt.polar(angles, values, label=f\"Cluster {idx}\")\n",
    "\n",
    "        # Afficher les vraies moyennes en annotation\n",
    "        for j, var in enumerate(categories):\n",
    "            angle = angles[j]\n",
    "            radius = row[var]\n",
    "            orig_val = original_means.loc[idx, var]\n",
    "            offset = 0.05 if radius >= 0 else -0.05\n",
    "            plt.text(\n",
    "                angle,\n",
    "                radius + offset,\n",
    "                f\"{orig_val:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=10,\n",
    "                color=\"black\",\n",
    "            )\n",
    "        print(idx)\n",
    "\n",
    "    plt.xticks(angles[:-1], categories, color=\"grey\", size=12)\n",
    "    plt.title(\"Radar des centroïdes (standardisés)\")\n",
    "    plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_radar(centroids_df=centroids, data_orig_df=data_chosen, labels=final_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de7af5",
   "metadata": {},
   "source": [
    "#### data_RFM :\n",
    "Les 4 clusters se distinguent seulement par leur dépense totale moyenne\n",
    "\n",
    "#### data_numeric : \n",
    "\n",
    "- Cluster 0 : clients satisfaits, qui achètent peu mais des produits chers\n",
    "- Cluster 1 : dépensiers moyens mais particulièrement insatisfaits\n",
    "- Cluster 2 : parmi les plus dépensiers, qui achètent beaucoup de produits pas chers. Leur satisfaction est plutot moyenne\n",
    "- Cluster 3 : clients économes et satisfaits\n",
    "\n",
    "\n",
    "#### Conclusion \n",
    "La division en clusters à partir de data_numeric semble avoir plus de sens et porter plus d'informations métier que pour data_RFM. Malgré ses scores moindre, on choisit donc le dataset_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed01191",
   "metadata": {},
   "source": [
    "## Comparer la distribution de chaque variable selon le cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chosen[\"cluster\"] = final_kmeans.labels_\n",
    "\n",
    "# Pour chaque variable quantitative\n",
    "for col in data_chosen.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=\"cluster\", y=col, data=data_chosen)\n",
    "    plt.title(f\"Distribution de {col} par cluster\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e9de5",
   "metadata": {},
   "source": [
    "## Visualiser les clusters en 2D grâce à une ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b352d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X_transformed)\n",
    "pca_df = pd.DataFrame(components, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"cluster\"] = final_kmeans.labels_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"cluster\", palette=\"Set2\")\n",
    "plt.title(\"ACP - Visualisation des clusters\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0] * 100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1] * 100:.1f}%)\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614f7e2",
   "metadata": {},
   "source": [
    "# DBSCAN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e8705",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de clustering basé sur la densité.\n",
    "Il regroupe les points densément connectés et identifie les points isolés comme du bruit --> le nombre de clusters n'est pas un paramètre, il émerge naturellement\n",
    "Il nécessite deux paramètres :\n",
    "- eps : la distance maximale entre deux points pour qu'ils soient considérés comme voisins.\n",
    "- min_samples : le nombre minimum de points pour former un noyau dense (core point).\n",
    "\n",
    "Fonctionnement :\n",
    "1. Pour chaque point, DBSCAN récupère ses voisins dans un rayon eps.\n",
    "2. Si le point a au moins 'min_samples' voisins, il devient un point noyau et forme un cluster.\n",
    "3. Les voisins directs et indirects (par transitivité) sont ajoutés au cluster.\n",
    "4. Les points trop isolés (pas assez de voisins) sont marqués comme bruit.\n",
    "Avantages : détecte des clusters de forme arbitraire et gère bien le bruit.\n",
    "Inconvénients : sensible au choix des paramètres et moins performant lorsque les densités sont très variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c7002",
   "metadata": {},
   "source": [
    "D'après [cette source](https://stackoverflow.com/questions/15050389/estimating-choosing-optimal-hyperparameters-for-dbscan/15063143#15063143), la valeur optimale de min_samples est 2xnb_features ici et la valeur optimale de eps est définie par un coude dans la courbe du classement croissant des distances des K plus proches voisins (K = 2xnb_features-1) : les distances augmentent d'un coup pour les points correspondant à du bruit. Le code ci-dessous permet de tracer cette courbe et de définir eps : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d24418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kdist_plot(X=None, k=None, radius_nbrs=1.0):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, radius=radius_nbrs).fit(X)\n",
    "\n",
    "    # For each point, compute distances to its k-nearest neighbors\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:, k - 1]\n",
    "\n",
    "    # Plot the sorted K-nearest neighbor distance for each point in the dataset\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel(\"Points/Objects in the dataset\", fontsize=12)\n",
    "    plt.ylabel(\"Sorted {}-nearest neighbor distance\".format(k), fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", color=\"black\", alpha=0.4)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "pt = PowerTransformer(method=\"yeo-johnson\", standardize = True)\n",
    "## Sur les variables numériques : \n",
    "X_transformed = pt.fit_transform(data_numeric)\n",
    "k = 2 * X_transformed.shape[-1] - 1  # k=2*{dim(dataset)} - 1\n",
    "get_kdist_plot(X=X_transformed, k=k)\n",
    "## Sur toutes les variables\n",
    "X_transformed = pt.fit_transform(data_tot)\n",
    "k = 2 * X_transformed.shape[-1] - 1  # k=2*{dim(dataset)} - 1\n",
    "get_kdist_plot(X=X_transformed, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba61f89",
   "metadata": {},
   "source": [
    "D'après ces courbes, la valeur optimale de eps est : \n",
    "- 0.005 pour data_numeric  \n",
    "- 1 pour data_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f36ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(\n",
    "    eps=0.5, min_samples=10, metric=\"euclidean\", algorithm=\"ball_tree\"\n",
    ").fit(X_transformed)\n",
    "clusters = clustering.labels_\n",
    "print(f\"Avec les paramètres choisis, DBSCAN a défini {len(set(clusters))} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb589e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taux d'outliers\n",
    "mask = (clusters == -1)\n",
    "percent_outliers = np.round(len(X_transformed[mask]) / len(X_transformed) * 100, 4)\n",
    "print(f\"Il y a {percent_outliers}% d'outliers\")\n",
    "\n",
    "# Enlever les points marqués comme bruit (label = -1)\n",
    "mask = clusters != -1\n",
    "X_WO_outliers = X_transformed[mask]\n",
    "labels_WO_outliers = clusters[mask]\n",
    "\n",
    "# Score spécifique auxalgorithmes de densité\n",
    "validity = validity_index(X_WO_outliers, labels_WO_outliers, metric=\"euclidean\")\n",
    "print(f\"Density-based cluster validity : {validity}\")\n",
    "\n",
    "# Moyennes par cluster\n",
    "unique_labels = np.unique(labels_WO_outliers)\n",
    "cluster_means = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    cluster_points = X_WO_outliers[labels_WO_outliers == label]\n",
    "    cluster_means.append(cluster_points.mean(axis=0))\n",
    "\n",
    "cluster_means = np.array(cluster_means)\n",
    "\n",
    "# Radar plot\n",
    "num_vars = X_WO_outliers.shape[1]\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # fermeture du polygone\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "for i, row in enumerate(cluster_means):\n",
    "    row_closed = np.concatenate([row, [row[0]]])\n",
    "    ax.plot(angles, row_closed, label=f\"Cluster {unique_labels[i]}\")\n",
    "    ax.fill(angles, row_closed, alpha=0.2)\n",
    "\n",
    "# Labels des axes\n",
    "feature_labels = [f\"{data_chosen.columns[i]}\" for i in range(num_vars)]\n",
    "angles_labels = angles[:-1]  # enlever l'angle du doublon\n",
    "ax.set_xticks(angles_labels)\n",
    "ax.set_xticklabels(feature_labels)\n",
    "\n",
    "ax.set_title(\"Profil moyen par cluster (DBSCAN)\", y=1.08)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.2, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3831b",
   "metadata": {},
   "source": [
    "#### DBSCAN sur data_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703d072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Variables numériques seules\n",
      "Avec les paramètres choisis, DBSCAN a défini 359 clusters\n",
      "Il y a 10.489% d'outliers\n"
     ]
    }
   ],
   "source": [
    "prepare_compute_evaluate_dbscan(data = datasets_list[1], data_description = datasets_descriptions[1], eps = 0.005, min_samples = 2*len(datasets_list[1].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502989b",
   "metadata": {},
   "source": [
    "Avec ces valeurs de paramètres, on obtient 359 clusters. Autant de clusters empêche l'interprétation des résultats. On garde donc la valeur par défaut (0.5) qui donne 4 clusters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_compute_evaluate_dbscan(data = datasets_list[1], data_description = datasets_descriptions[1], eps = 0.5, min_samples = 2*len(datasets_list[1].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e820a",
   "metadata": {},
   "source": [
    "#### DBSCAN sur data_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_compute_evaluate_dbscan(data = datasets_list[-1], data_description = datasets_descriptions[-1], eps = 1, min_samples = 2*len(datasets_list[1].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b07f718",
   "metadata": {},
   "source": [
    "# Clustering hiérarchique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1b164",
   "metadata": {},
   "source": [
    "Le clustering hiérarchique regroupe les points par similarité de manière progressive.\n",
    "Il commence avec chaque point dans son propre cluster, puis fusionne les plus proches (approche agglomérative).\n",
    "Le processus continue jusqu’à obtenir un seul cluster ou un nombre défini de clusters.\n",
    "La proximité entre clusters peut être mesurée par différents critères (moyenne, minimum, Ward i.e. regrouppement entrainant la plus faible augmentation d'inertie).\n",
    "Le résultat peut être visualisé sous forme de dendrogramme (arbre de fusions).\n",
    "Il n’est pas nécessaire de définir le nombre de clusters à l’avance : on peut couper l’arbre à différents niveaux.\n",
    "Peut créer des problèmes de mémoire RAM donc on fait un K means grossier en amont, afin de se ramener à 10 000 clusters (technique de réduction du nombre de lignes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5c98b",
   "metadata": {},
   "source": [
    "peut créer des pb de mémoire RAM. K means grossier --> 1000 clusters = technique de réduction du nombre de lignes. Puis clustering hiérarchique sur ces clusters. Ensuite choisir le nb de cluster sur la base de silhouette et autre métriques utilisées sur k means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f340066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans préliminaire - réduction du nombre de lignes\n",
    "preliminary_kmeans = KMeans(n_clusters=10_000, random_state=42)\n",
    "preliminary_cluster_labels = preliminary_kmeans.fit_predict(X_transformed)\n",
    "\n",
    "# silhouette_avg = silhouette_score(X_transformed, preliminary_cluster_labels)\n",
    "# db_avg = davies_bouldin_score(X_transformed, preliminary_cluster_labels)\n",
    "# inertia = preliminary_kmeans.inertia_\n",
    "\n",
    "# print(\n",
    "#     f\"Inertie : {inertia} \\nSilhouette : {silhouette_avg} \\nDavies Bouldin : {db_avg}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering hiérarchique par approche agglomérative\n",
    "\n",
    "centroids = pd.DataFrame(\n",
    "    preliminary_kmeans.cluster_centers_, columns=data_chosen.columns\n",
    ")\n",
    "\n",
    "#Calcul des liaisons hiérarchiques\n",
    "Z = linkage(centroids, method=\"ward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9971d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "_ = dendrogram(Z, p=10, truncate_mode=\"lastp\", ax=ax)\n",
    "\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.ylabel(\"Distance.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2, 11):\n",
    "    labels = fcluster(Z, k, criterion=\"maxclust\")\n",
    "    silhouette_avg = silhouette_score(preliminary_kmeans, labels)\n",
    "    db_avg = davies_bouldin_score(preliminary_kmeans, labels)\n",
    "    print(f\"Silhouette : {silhouette_avg} \\nDavies Bouldin : {db_avg}\")\n",
    "    score = silhouette_score(X, labels)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c892ddf",
   "metadata": {},
   "source": [
    "Modifier le code ci dessus pour attribuer le cluster à chaque objet issu du kmeans. \n",
    "On peut aussi évaluer la cohérence de l'arbre pour choisir le nombre de clusters --> implémenté ici : https://docs.scipy.org/doc/scipy-1.15.2/reference/generated/scipy.cluster.hierarchy.inconsistent.html Il existe d'autres méthodes appropriées à l'arbre : https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.cut_tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc9d25",
   "metadata": {},
   "source": [
    "Critère de choix du modèle : l'interprétabilité et stabilité. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
